{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"\n",
    "    Obtain Bemis-Murcko scaffold from smiles\n",
    "    :param smiles:\n",
    "    :param include_chirality:\n",
    "    :return: smiles of scaffold\n",
    "    \"\"\"\n",
    "    scaffold = MurckoScaffold.MurckoScaffoldSmiles(\n",
    "        smiles=smiles, includeChirality=include_chirality)\n",
    "    return scaffold\n",
    "\n",
    "\n",
    "# # test generate_scaffold\n",
    "# s = 'Cc1cc(Oc2nccc(CCC)c2)ccc1'\n",
    "# scaffold = generate_scaffold(s)\n",
    "# assert scaffold == 'c1ccc(Oc2ccccn2)cc1'\n",
    "\n",
    "def scaffold_split(dataset, smiles_list, task_idx=None, null_value=0,\n",
    "                   frac_train=0.8, frac_valid=0.1, frac_test=0.1, ):\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)\n",
    "\n",
    "    if task_idx != None:\n",
    "        # filter based on null values in task_idx\n",
    "        # get task array\n",
    "        y_task = np.array([data.y[task_idx].item() for data in dataset])\n",
    "        # boolean array that correspond to non null values\n",
    "        non_null = y_task != null_value\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "    else:\n",
    "        non_null = np.ones(len(dataset)) == 1\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "\n",
    "    scaffolds_list = []\n",
    "    for i, smiles in smiles_list:\n",
    "        scaffold = generate_scaffold(smiles, include_chirality=True)\n",
    "        scaffolds_list.append(scaffold)\n",
    "\n",
    "    dataset['scaffold'] = scaffolds_list\n",
    "    fps = dataset['scaffold'].apply(lambda x: AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(x), 2, nBits=1024))\n",
    "    print(\"fingerprint completed\")\n",
    "    fps = fps.tolist()\n",
    "    dists = []\n",
    "    nfps = len(fps)\n",
    "    for i in tqdm(range(1, nfps), desc=\"Calculating similarities\"):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n",
    "        dists.extend([1 - x for x in sims])\n",
    "    print(\"start clutering\")\n",
    "    clusters = Butina.ClusterData(dists, nfps, 0.4, isDistData=True)\n",
    "    cluster_labels = np.zeros(nfps, dtype=int)\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for idx in cluster:\n",
    "            cluster_labels[idx] = cluster_id + 1\n",
    "    dataset['cluster'] = cluster_labels\n",
    "    print(\"cluter completed\")\n",
    "    \n",
    "    # create dict of the form {cluster1: [idx1, idx....]}\n",
    "    all_clusters = {}\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        if label not in all_clusters:\n",
    "            all_clusters[label] = [i]\n",
    "        else:\n",
    "            all_clusters[label].append(i)\n",
    "\n",
    "    # sort from largest to smallest sets\n",
    "    all_clusters = {key: sorted(value) for key, value in all_clusters.items()}\n",
    "    all_clusters_sets = [\n",
    "        clusters_set for (clusters, clusters_set) in sorted(\n",
    "            all_clusters.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)\n",
    "    ]\n",
    "\n",
    "    # get train, valid test indices\n",
    "    train_cutoff = frac_train * len(cluster_labels)\n",
    "    valid_cutoff = (frac_train + frac_valid) * len(cluster_labels)\n",
    "    train_idx, valid_idx, test_idx = [], [], []\n",
    "    for clusters_set in all_clusters_sets:\n",
    "        if len(train_idx) + len(clusters_set) > train_cutoff:\n",
    "            if len(train_idx) + len(valid_idx) + len(clusters_set) > valid_cutoff:\n",
    "                test_idx.extend(clusters_set)\n",
    "            else:\n",
    "                valid_idx.extend(clusters_set)\n",
    "        else:\n",
    "            train_idx.extend(clusters_set)\n",
    "\n",
    "    assert len(set(train_idx).intersection(set(valid_idx))) == 0\n",
    "    assert len(set(test_idx).intersection(set(valid_idx))) == 0\n",
    "    \n",
    "    train_dataset = dataset.iloc[train_idx]\n",
    "    valid_dataset = dataset.iloc[valid_idx]\n",
    "    test_dataset = dataset.iloc[test_idx]\n",
    "    return train_dataset,valid_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50016\n",
      "fingerprint completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating similarities: 100%|██████████| 50015/50015 [11:49<00:00, 70.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start clutering\n",
      "cluter completed\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/uspto50k/raw/uspto50k_train_application.csv',index_col=False)\n",
    "df_val = pd.read_csv('data/uspto50k/raw/uspto50k_val_application.csv',index_col=False)\n",
    "df_test = pd.read_csv('data/uspto50k/raw/uspto50k_test_application.csv',index_col=False)\n",
    "dataset = pd.concat([df_train, df_val,df_test], ignore_index=True)\n",
    "smiles_list = dataset['reactants>reagents>production'].apply(lambda x: x.split('>')[2])\n",
    "print(len(dataset))\n",
    "train_dataset,valid_dataset,test_dataset = scaffold_split(dataset, smiles_list, task_idx=None, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "train_dataset.to_csv('data/uspto50k/raw/uspto50k_train_scaffold.csv', index=False)\n",
    "valid_dataset.to_csv('data/uspto50k/raw/uspto50k_val_scaffold.csv', index=False)\n",
    "test_dataset.to_csv('data/uspto50k/raw/uspto50k_test_scaffold.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "retreb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
